{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deva-kumari/devakumari_cse22237/blob/main/principles%20of%20ai%20code%20without%20embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3DFcwVQSZKp",
        "outputId": "317b4d32-4f2b-4672-82ed-137ad88cbacb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the uploaded zip file\n",
        "zip_path = '/content/data.zip'\n",
        "extracted_path = '/content/data/data_extracted/'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# List the extracted files\n",
        "extracted_files = os.listdir(extracted_path)\n",
        "extracted_files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the ZIP file\n",
        "zip_path = \"/content/data.zip\"  # Replace with your actual path\n",
        "extracted_path = \"/content/extracted_data/\"\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# List the extracted files and directories\n",
        "for root, dirs, files in os.walk(extracted_path):\n",
        "    print(\"Root:\", root)\n",
        "    print(\"Directories:\", dirs)\n",
        "    print(\"Files:\", files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgO3XBJUdzj6",
        "outputId": "973670a6-9460-4477-9583-614a9b1c4f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root: /content/extracted_data/\n",
            "Directories: ['data']\n",
            "Files: []\n",
            "Root: /content/extracted_data/data\n",
            "Directories: ['neg', 'pos']\n",
            "Files: []\n",
            "Root: /content/extracted_data/data/neg\n",
            "Directories: []\n",
            "Files: ['10482_1.txt', '10170_2.txt', '10199_4.txt', '10050_2.txt', '10224_4.txt', '10196_1.txt', '10189_4.txt', '10003_1.txt', '10358_1.txt', '10158_4.txt', '10139_4.txt', '10136_2.txt', '10153_4.txt', '10426_1.txt', '10187_3.txt', '10209_3.txt', '10316_3.txt', '10260_1.txt', '10318_2.txt', '10133_1.txt', '1005_3.txt', '10141_2.txt', '10088_1.txt', '10223_4.txt', '10235_2.txt', '10183_1.txt', '10402_1.txt', '10016_4.txt', '1039_1.txt', '10483_4.txt', '10448_1.txt', '10177_1.txt', '10432_2.txt', '10089_1.txt', '10440_1.txt', '10324_2.txt', '10090_1.txt', '10218_3.txt', '10229_3.txt', '1009_4.txt', '10125_1.txt', '10391_3.txt', '10072_1.txt', '10263_4.txt', '10385_3.txt', '1030_2.txt', '10265_2.txt', '10320_2.txt', '10054_1.txt', '1016_4.txt', '10396_1.txt', '10027_1.txt', '10356_2.txt', '10256_3.txt', '10447_1.txt', '10093_1.txt', '10295_1.txt', '10057_1.txt', '10377_1.txt', '10049_1.txt', '1046_2.txt', '1003_3.txt', '10439_1.txt', '10394_3.txt', '10434_3.txt', '10282_2.txt', '10168_2.txt', '10129_1.txt', '10301_4.txt', '1018_3.txt', '10296_1.txt', '10142_2.txt', '10061_4.txt', '10378_2.txt', '10131_1.txt', '10040_2.txt', '10276_1.txt', '10086_2.txt', '10323_1.txt', '10150_3.txt', '1024_1.txt', '10273_2.txt', '10480_2.txt', '10017_4.txt', '1040_1.txt', '10169_2.txt', '10311_3.txt', '10332_1.txt', '1011_2.txt', '10438_4.txt', '10472_1.txt', '1012_2.txt', '10239_3.txt', '10246_2.txt', '10161_2.txt', '10098_1.txt', '10373_3.txt', '10259_1.txt', '10411_2.txt', '10292_3.txt', '10167_1.txt', '10319_1.txt', '10283_1.txt', '10011_3.txt', '10255_1.txt', '10474_3.txt', '10182_3.txt', '10236_1.txt', '10266_1.txt', '10471_2.txt', '10349_4.txt', '10253_1.txt', '10254_1.txt', '10010_3.txt', '10269_1.txt', '10453_1.txt', '1042_1.txt', '10415_2.txt', '1001_4.txt', '10431_1.txt', '10450_1.txt', '10404_4.txt', '10219_2.txt', '10353_3.txt', '10080_2.txt', '10279_3.txt', '10237_2.txt', '10421_2.txt', '10333_1.txt', '10451_1.txt', '10285_3.txt', '10372_3.txt', '10175_3.txt', '10163_4.txt', '10315_2.txt', '10149_1.txt', '10043_1.txt', '10326_1.txt', '10242_3.txt', '10248_1.txt', '10317_1.txt', '1023_3.txt', '10006_4.txt', '10024_3.txt', '10066_4.txt', '10001_4.txt', '1037_1.txt', '10387_1.txt', '1031_2.txt', '10414_3.txt', '10386_4.txt', '1029_3.txt', '10103_1.txt', '10178_3.txt', '10465_2.txt', '10059_4.txt', '10077_2.txt', '10204_2.txt', '10312_4.txt', '10460_3.txt', '10012_1.txt', '10106_1.txt', '10193_1.txt', '1041_1.txt', '10164_4.txt', '10280_3.txt', '10055_3.txt', '10479_2.txt', '10127_1.txt', '10270_1.txt', '10026_2.txt', '10030_1.txt', '10443_3.txt', '10051_4.txt', '10455_1.txt', '1032_1.txt', '10390_2.txt', '10073_4.txt', '10252_3.txt', '10367_4.txt', '10335_3.txt', '10345_4.txt', '10015_2.txt', '10228_3.txt', '10247_2.txt', '10117_1.txt', '10147_1.txt', '10071_1.txt', '10079_1.txt', '10062_1.txt', '10437_4.txt', '10418_1.txt', '10478_3.txt', '1010_4.txt', '10272_1.txt', '10126_2.txt', '10401_2.txt', '10397_2.txt', '10351_1.txt', '10067_3.txt', '10105_1.txt', '10063_1.txt', '10078_1.txt', '10362_1.txt', '10234_1.txt', '10245_3.txt', '10430_3.txt', '10132_2.txt', '10271_1.txt', '10081_1.txt', '10213_1.txt', '10068_4.txt', '1006_1.txt', '10018_3.txt', '10159_3.txt', '10192_2.txt', '10425_3.txt', '10399_4.txt', '10348_1.txt', '1027_1.txt', '10400_1.txt', '101_1.txt', '10407_4.txt', '10476_1.txt', '10108_1.txt', '10100_1.txt', '10022_4.txt', '10405_4.txt', '10029_1.txt', '10201_3.txt', '10180_1.txt', '10120_4.txt', '1019_1.txt', '10060_1.txt', '10099_1.txt', '10380_4.txt', '10293_4.txt', '10240_1.txt', '10025_1.txt', '10310_2.txt', '10475_1.txt', '10114_1.txt', '10023_1.txt', '10398_3.txt', '1002_3.txt', '10412_1.txt', '10417_1.txt', '10181_3.txt', '10092_1.txt', '10231_1.txt', '10336_4.txt', '10084_4.txt', '10042_1.txt', '10382_4.txt', '10429_1.txt', '10185_4.txt', '10304_1.txt', '10035_1.txt', '10473_3.txt', '10207_2.txt', '10220_3.txt', '10384_4.txt', '1021_1.txt', '10309_3.txt', '1025_2.txt', '10463_1.txt', '10013_1.txt', '10009_1.txt', '10156_1.txt', '10160_4.txt', '10334_3.txt', '10020_3.txt', '10469_1.txt', '10215_2.txt', '10420_2.txt', '10109_1.txt', '10485_1.txt', '10435_2.txt', '10424_2.txt', '10198_4.txt', '10135_2.txt', '10413_1.txt', '10302_1.txt', '10032_4.txt', '10033_1.txt', '1015_2.txt', '10277_1.txt', '10005_3.txt', '10152_1.txt', '10151_1.txt', '10205_2.txt', '10157_4.txt', '10226_3.txt', '10097_1.txt', '10257_1.txt', '10441_1.txt', '10361_1.txt', '10172_1.txt', '10481_2.txt', '10104_1.txt', '10144_2.txt', '10422_1.txt', '10449_4.txt', '10031_2.txt', '10410_1.txt', '10375_1.txt', '10162_3.txt', '10148_3.txt', '10176_1.txt', '10299_4.txt', '10128_1.txt', '10251_3.txt', '10436_4.txt', '10110_1.txt', '10238_4.txt', '10002_1.txt', '10416_2.txt', '10374_1.txt', '10111_1.txt', '10313_4.txt', '10352_4.txt', '10305_4.txt', '10036_1.txt', '10112_1.txt', '10233_1.txt', '1008_1.txt', '0_3.txt', '10190_1.txt', '10267_1.txt', '100_3.txt', '10357_1.txt', '10154_1.txt', '10217_1.txt', '10470_1.txt', '10346_4.txt', '10146_1.txt', '10039_1.txt', '10466_2.txt', '10278_1.txt', '1036_3.txt', '10052_4.txt', '10355_1.txt', '1004_4.txt', '10461_1.txt', '10287_4.txt', '10288_1.txt', '10222_4.txt', '10014_2.txt', '10268_1.txt', '10464_1.txt', '10467_1.txt', '10130_2.txt', '10225_3.txt', '10452_1.txt', '10331_2.txt', '10338_2.txt', '10028_2.txt', '10004_3.txt', '10457_3.txt', '10082_4.txt', '10045_1.txt', '10477_1.txt', '10383_2.txt', '10064_3.txt', '10300_3.txt', '1013_2.txt', '10186_1.txt', '10069_1.txt', '10419_1.txt', '10140_3.txt', '10364_4.txt', '10329_1.txt', '10459_2.txt', '1045_2.txt', '10433_4.txt', '10243_1.txt', '10041_1.txt', '10363_2.txt', '1000_4.txt', '10058_1.txt', '10191_3.txt', '10392_3.txt', '10197_1.txt', '10166_2.txt', '10134_1.txt', '10325_1.txt', '10087_4.txt', '10274_4.txt', '10444_1.txt', '10195_1.txt', '10145_1.txt', '10230_1.txt', '1022_1.txt', '10359_1.txt', '10306_4.txt', '10174_4.txt', '10409_2.txt', '10188_2.txt', '10344_4.txt', '10281_1.txt', '1034_2.txt', '10206_3.txt', '10446_2.txt', '1020_2.txt', '10083_1.txt', '10121_3.txt', '1017_1.txt', '10381_4.txt', '10328_4.txt', '1028_3.txt', '10337_3.txt', '10232_1.txt', '10212_1.txt', '10053_4.txt', '1038_2.txt', '10275_3.txt', '10214_4.txt', '10258_4.txt', '10341_1.txt', '10365_1.txt', '10194_3.txt', '10484_2.txt', '10119_2.txt', '10289_1.txt', '10019_3.txt', '102_1.txt', '10070_3.txt', '10445_1.txt', '10115_1.txt', '1044_4.txt', '10368_1.txt', '10250_3.txt', '10308_1.txt', '10074_2.txt', '10165_3.txt', '10113_1.txt', '10116_1.txt', '10179_1.txt', '1033_4.txt', '10342_2.txt', '1035_1.txt', '10487_2.txt', '10038_3.txt', '10314_4.txt', '10171_3.txt', '10044_1.txt', '10124_1.txt', '10143_1.txt', '10406_3.txt', '10249_1.txt', '10056_2.txt', '1043_3.txt', '10216_3.txt', '10107_1.txt', '10037_1.txt', '10389_4.txt', '10046_1.txt', '10468_1.txt', '10343_3.txt', '10048_4.txt', '10395_2.txt', '10291_4.txt', '10065_2.txt', '10371_3.txt', '10203_2.txt', '10286_1.txt', '10221_3.txt', '10034_1.txt', '10403_2.txt', '10095_1.txt', '10284_3.txt', '10076_4.txt', '10330_1.txt', '10208_3.txt', '10075_2.txt', '10388_4.txt', '10173_1.txt', '1047_2.txt', '10298_4.txt', '10137_1.txt', '10094_1.txt', '10428_4.txt', '10200_4.txt', '10486_3.txt', '10408_1.txt', '10000_4.txt', '10297_3.txt', '10290_1.txt', '10102_1.txt', '10211_2.txt', '1026_3.txt', '10091_1.txt', '10021_2.txt', '10122_4.txt', '10366_1.txt', '10101_1.txt', '1014_2.txt', '10376_1.txt', '10427_4.txt', '10321_2.txt', '10354_4.txt', '10202_3.txt', '10423_1.txt', '10047_1.txt', '10454_1.txt', '10442_2.txt', '10262_1.txt', '1007_1.txt', '10184_1.txt', '10322_2.txt', '10458_2.txt', '10370_4.txt', '10340_1.txt', '10307_2.txt', '10369_1.txt', '10227_1.txt', '10327_1.txt', '10241_1.txt', '103_1.txt', '10244_2.txt', '10007_1.txt', '10096_1.txt', '10339_2.txt', '10347_4.txt', '10118_4.txt', '10379_2.txt', '10210_3.txt', '10155_4.txt', '10456_1.txt', '10462_4.txt', '10294_1.txt', '10261_1.txt', '10138_3.txt', '10123_3.txt', '10303_1.txt', '10350_1.txt', '10393_4.txt', '10264_4.txt', '10085_3.txt', '10008_2.txt', '10360_3.txt']\n",
            "Root: /content/extracted_data/data/pos\n",
            "Directories: []\n",
            "Files: ['10586_10.txt', '10626_7.txt', '10456_10.txt', '10029_10.txt', '10308_8.txt', '10282_8.txt', '1057_9.txt', '10449_9.txt', '10623_8.txt', '10591_10.txt', '10578_7.txt', '10156_10.txt', '1078_8.txt', '1112_8.txt', '10448_10.txt', '1086_7.txt', '1082_10.txt', '10385_10.txt', '10157_10.txt', '1096_9.txt', '10450_10.txt', '1070_8.txt', '10573_10.txt', '10005_7.txt', '10084_10.txt', '10508_10.txt', '10341_7.txt', '10547_9.txt', '10395_8.txt', '10277_9.txt', '10053_8.txt', '10641_7.txt', '10119_7.txt', '10343_7.txt', '10293_8.txt', '10034_8.txt', '1021_10.txt', '10470_9.txt', '10378_8.txt', '10126_10.txt', '10345_7.txt', '10038_10.txt', '10315_8.txt', '10579_10.txt', '1083_10.txt', '10244_7.txt', '1020_10.txt', '1019_10.txt', '10619_8.txt', '10361_7.txt', '10239_10.txt', '1058_10.txt', '10317_7.txt', '10481_8.txt', '10290_8.txt', '10080_10.txt', '10145_8.txt', '10531_10.txt', '10197_7.txt', '10023_9.txt', '10486_7.txt', '1101_8.txt', '10581_10.txt', '10279_8.txt', '1093_8.txt', '10054_10.txt', '10190_7.txt', '10138_8.txt', '10437_7.txt', '10294_8.txt', '10111_7.txt', '10512_10.txt', '10459_9.txt', '10414_10.txt', '10630_8.txt', '10546_9.txt', '10376_7.txt', '10541_10.txt', '10043_10.txt', '1090_8.txt', '10233_7.txt', '1115_9.txt', '10567_9.txt', '10022_7.txt', '10027_7.txt', '10026_7.txt', '10331_10.txt', '10312_10.txt', '10454_9.txt', '10068_8.txt', '1043_10.txt', '10002_7.txt', '10394_10.txt', '1076_8.txt', '10177_9.txt', '1023_10.txt', '10377_9.txt', '10369_8.txt', '10473_10.txt', '10183_7.txt', '10163_8.txt', '1069_10.txt', '10406_10.txt', '10356_9.txt', '10220_7.txt', '10281_7.txt', '10172_8.txt', '10505_10.txt', '10350_10.txt', '10329_8.txt', '10577_10.txt', '10144_8.txt', '10256_8.txt', '10368_7.txt', '10064_10.txt', '10297_8.txt', '10597_9.txt', '10387_7.txt', '10049_8.txt', '10520_9.txt', '10498_10.txt', '10542_7.txt', '10155_9.txt', '10218_8.txt', '1088_9.txt', '10446_10.txt', '10067_9.txt', '10528_10.txt', '100_7.txt', '10570_8.txt', '10078_8.txt', '10625_7.txt', '1016_8.txt', '10167_7.txt', '10188_8.txt', '10384_10.txt', '10131_10.txt', '10_9.txt', '10463_10.txt', '10132_9.txt', '10602_10.txt', '10432_10.txt', '10444_9.txt', '10405_8.txt', '10299_9.txt', '10127_8.txt', '10536_10.txt', '10466_8.txt', '10382_10.txt', '10003_8.txt', '1007_10.txt', '10311_9.txt', '10191_10.txt', '10420_10.txt', '10504_9.txt', '10041_10.txt', '10098_10.txt', '10042_10.txt', '10113_10.txt', '10627_10.txt', '10182_8.txt', '10305_8.txt', '10522_7.txt', '10408_10.txt', '10515_9.txt', '10296_8.txt', '10603_10.txt', '10439_8.txt', '10307_8.txt', '1085_7.txt', '10469_10.txt', '10379_10.txt', '10097_9.txt', '10209_7.txt', '1035_7.txt', '10472_7.txt', '1108_7.txt', '10519_9.txt', '10291_7.txt', '10556_10.txt', '10075_9.txt', '1045_8.txt', '10415_7.txt', '10136_7.txt', '10372_7.txt', '10154_8.txt', '10606_10.txt', '10100_10.txt', '10184_9.txt', '10250_10.txt', '10231_10.txt', '10036_8.txt', '10168_8.txt', '10302_9.txt', '10224_10.txt', '10325_10.txt', '10052_10.txt', '10582_10.txt', '10009_9.txt', '1031_10.txt', '10170_8.txt', '10030_10.txt', '10390_10.txt', '10318_7.txt', '10540_10.txt', '10401_10.txt', '10181_8.txt', '10288_10.txt', '10090_8.txt', '10107_8.txt', '10040_10.txt', '10614_7.txt', '1015_10.txt', '10511_7.txt', '10435_7.txt', '10494_10.txt', '10303_7.txt', '1030_10.txt', '10051_10.txt', '10534_7.txt', '10263_10.txt', '10192_8.txt', '10588_10.txt', '10624_7.txt', '1034_7.txt', '10241_8.txt', '10150_9.txt', '10186_8.txt', '10400_10.txt', '10410_10.txt', '10021_8.txt', '10010_7.txt', '10632_10.txt', '10362_8.txt', '10304_7.txt', '10004_8.txt', '10629_10.txt', '10248_7.txt', '10358_9.txt', '10149_9.txt', '10270_9.txt', '10389_10.txt', '10313_7.txt', '10203_10.txt', '1004_7.txt', '0_9.txt', '10102_7.txt', '10328_8.txt', '10527_10.txt', '10635_8.txt', '1100_7.txt', '10243_10.txt', '10326_10.txt', '10205_10.txt', '10354_9.txt', '1098_9.txt', '10020_8.txt', '10088_10.txt', '10140_8.txt', '1006_8.txt', '1008_10.txt', '1056_10.txt', '10426_9.txt', '1037_8.txt', '10599_8.txt', '1067_7.txt', '10179_9.txt', '10166_7.txt', '10370_9.txt', '10555_8.txt', '10639_7.txt', '1024_9.txt', '10628_7.txt', '10165_7.txt', '10605_7.txt', '1065_10.txt', '10594_8.txt', '10521_9.txt', '10320_7.txt', '10058_7.txt', '10595_10.txt', '10340_9.txt', '1080_9.txt', '1046_10.txt', '10561_8.txt', '10492_10.txt', '10518_9.txt', '10396_8.txt', '10460_10.txt', '1050_9.txt', '10644_8.txt', '10035_9.txt', '10240_8.txt', '10324_9.txt', '10633_9.txt', '10388_7.txt', '10523_9.txt', '10287_8.txt', '1110_9.txt', '10436_8.txt', '10258_10.txt', '10204_8.txt', '10278_7.txt', '10202_10.txt', '10211_7.txt', '10553_8.txt', '1071_8.txt', '10611_8.txt', '10596_8.txt', '10216_8.txt', '10416_9.txt', '10493_9.txt', '10063_9.txt', '10285_10.txt', '10212_8.txt', '10121_8.txt', '10615_8.txt', '10365_8.txt', '102_10.txt', '10338_9.txt', '10158_7.txt', '10008_7.txt', '10447_10.txt', '10292_7.txt', '10496_10.txt', '10442_10.txt', '10268_9.txt', '1103_10.txt', '10535_7.txt', '10129_7.txt', '10342_7.txt', '1061_10.txt', '10526_9.txt', '1009_8.txt', '1087_10.txt', '10085_10.txt', '10538_8.txt', '10503_10.txt', '10576_7.txt', '10333_8.txt', '1059_10.txt', '10568_10.txt', '10643_8.txt', '10146_7.txt', '10565_9.txt', '106_10.txt', '10316_8.txt', '10616_7.txt', '10495_7.txt', '10506_10.txt', '10479_10.txt', '10510_7.txt', '10642_8.txt', '1102_8.txt', '10585_9.txt', '1018_8.txt', '10458_10.txt', '10055_7.txt', '10457_8.txt', '10298_9.txt', '10011_9.txt', '10640_8.txt', '10225_9.txt', '10103_8.txt', '10210_7.txt', '1039_9.txt', '10123_10.txt', '10266_9.txt', '10631_8.txt', '10104_10.txt', '1047_8.txt', '10428_10.txt', '10434_10.txt', '1022_10.txt', '1105_8.txt', '1011_10.txt', '10551_7.txt', '1052_8.txt', '10393_9.txt', '10413_10.txt', '10618_8.txt', '105_7.txt', '10440_9.txt', '10117_8.txt', '10142_8.txt', '10228_8.txt', '1036_9.txt', '10247_10.txt', '1114_8.txt', '10646_8.txt', '10013_7.txt', '1002_7.txt', '10069_8.txt', '10429_10.txt', '10500_10.txt', '10334_8.txt', '10252_9.txt', '10024_9.txt', '10489_10.txt', '10124_8.txt', '10074_9.txt', '10284_9.txt', '10462_7.txt', '10213_8.txt', '10636_8.txt', '10398_8.txt', '10533_10.txt', '10236_8.txt', '10153_9.txt', '1111_10.txt', '10609_10.txt', '10257_8.txt', '10648_8.txt', '10419_10.txt', '10079_8.txt', '10198_8.txt', '10403_7.txt', '10006_7.txt', '1106_8.txt', '1001_8.txt', '10089_7.txt', '10569_10.txt', '10125_8.txt', '10355_9.txt', '10327_7.txt', '10227_10.txt', '1094_9.txt', '10180_7.txt', '10207_10.txt', '10222_9.txt', '10238_10.txt', '10208_7.txt', '10095_7.txt', '10028_10.txt', '10044_9.txt', '10424_9.txt', '10598_8.txt', '10273_8.txt', '1092_10.txt', '10032_10.txt', '10549_9.txt', '10584_10.txt', '10451_10.txt', '1040_10.txt', '10487_7.txt', '10497_8.txt', '10525_10.txt', '10502_9.txt', '101_8.txt', '10014_8.txt', '10607_10.txt', '10347_9.txt', '10116_10.txt', '10159_7.txt', '1033_10.txt', '10141_9.txt', '10147_10.txt', '1010_10.txt', '10559_8.txt', '10221_8.txt', '10206_10.txt', '10550_8.txt', '10402_10.txt', '10217_9.txt', '1014_9.txt', '10056_8.txt', '10360_8.txt', '1063_10.txt', '10482_10.txt', '10622_10.txt', '10062_10.txt', '10478_8.txt', '10404_9.txt', '10237_10.txt', '10110_10.txt', '10425_9.txt', '10048_10.txt', '10589_10.txt', '10545_7.txt', '10260_10.txt', '10373_7.txt', '10171_7.txt', '10060_9.txt', '10093_7.txt', '10174_7.txt', '10259_8.txt', '10094_7.txt', '10530_10.txt', '10031_10.txt', '1081_10.txt', '10574_10.txt', '10575_9.txt', '10300_10.txt', '10564_10.txt', '10484_8.txt', '1062_10.txt', '10421_7.txt', '1074_10.txt', '10480_10.txt', '10516_7.txt', '10351_8.txt', '10087_10.txt', '1000_8.txt', '10091_7.txt', '1042_10.txt', '10122_7.txt', '10164_7.txt', '1109_7.txt', '10475_8.txt', '10560_9.txt', '10397_8.txt', '10275_10.txt', '10490_7.txt', '1027_8.txt', '10283_10.txt', '10321_10.txt', '10223_10.txt', '10461_9.txt', '10418_9.txt', '10232_10.txt', '10610_8.txt', '10047_10.txt', '1072_10.txt', '10386_8.txt', '10187_7.txt', '10409_10.txt', '10474_9.txt', '10392_10.txt', '10558_10.txt', '10301_8.txt', '10476_9.txt', '10517_8.txt', '10310_9.txt', '10544_8.txt', '10507_10.txt', '10245_10.txt', '10501_10.txt', '1107_10.txt', '10375_10.txt', '1079_7.txt', '10065_9.txt', '10349_10.txt', '10001_10.txt', '1075_10.txt', '10120_7.txt', '1026_9.txt', '10306_8.txt', '1084_9.txt', '103_7.txt', '10152_9.txt', '10557_9.txt', '1041_9.txt', '10286_9.txt', '10464_7.txt', '10176_7.txt', '10269_7.txt', '10271_10.txt', '10621_10.txt', '10455_10.txt', '1068_10.txt', '10443_9.txt', '10323_10.txt', '10477_9.txt', '10109_10.txt', '10548_7.txt', '10471_10.txt', '10380_10.txt', '10514_8.txt', '10160_7.txt', '10246_10.txt', '10289_10.txt', '1104_8.txt', '1073_9.txt', '1028_10.txt', '10352_10.txt', '1038_7.txt', '10137_7.txt', '10371_8.txt', '10441_10.txt', '10267_8.txt', '10092_8.txt', '10115_10.txt', '10112_7.txt', '1025_8.txt', '10391_10.txt', '10076_9.txt', '1097_9.txt', '10359_7.txt', '10612_7.txt', '10336_8.txt', '1055_10.txt', '10017_9.txt', '10374_8.txt', '10201_10.txt', '10590_8.txt', '10485_8.txt', '10254_8.txt', '10081_9.txt', '1048_8.txt', '10381_10.txt', '10130_10.txt', '10007_7.txt', '107_10.txt', '10399_10.txt', '10083_7.txt', '10411_9.txt', '10509_7.txt', '10106_8.txt', '10433_9.txt', '10423_9.txt', '10407_8.txt', '10229_8.txt', '10066_10.txt', '10543_8.txt', '10059_10.txt', '10554_7.txt', '10265_9.txt', '10195_8.txt', '10242_8.txt', '10012_8.txt', '10638_8.txt', '10086_7.txt', '10601_10.txt', '10571_8.txt', '109_10.txt', '1003_10.txt', '10082_10.txt', '10430_9.txt', '10483_8.txt', '10572_8.txt', '10431_10.txt', '10357_8.txt', '10417_8.txt', '108_10.txt', '10070_9.txt', '10537_10.txt', '10647_8.txt', '10114_10.txt', '10162_9.txt', '10230_9.txt', '10033_10.txt', '10255_9.txt', '10046_9.txt', '10105_8.txt', '1077_8.txt', '1089_10.txt', '10488_10.txt', '10412_8.txt', '10234_10.txt', '10133_7.txt', '10077_10.txt', '10592_8.txt', '10169_7.txt', '1017_8.txt', '10016_8.txt', '10562_9.txt', '10332_8.txt', '10249_7.txt', '10604_7.txt', '10583_10.txt', '10096_7.txt', '10634_10.txt', '10563_7.txt', '10438_9.txt', '10314_8.txt', '10499_10.txt', '10194_10.txt', '1113_10.txt', '1051_9.txt', '1064_10.txt', '10148_10.txt', '10468_9.txt', '10566_8.txt', '10322_7.txt', '10196_10.txt', '10539_10.txt', '10465_8.txt', '10219_10.txt', '10274_8.txt', '10175_10.txt', '10251_10.txt', '10015_8.txt', '10608_10.txt', '10072_9.txt', '10134_7.txt', '10353_9.txt', '10193_9.txt', '1054_8.txt', '10587_8.txt', '1066_10.txt', '10337_9.txt', '10262_10.txt', '10330_8.txt', '1029_9.txt', '10173_8.txt', '10348_8.txt', '1095_9.txt', '10199_7.txt', '10253_10.txt', '10108_10.txt', '10039_10.txt', '10135_7.txt', '10580_8.txt', '10019_8.txt', '1012_10.txt', '10620_10.txt', '10161_9.txt', '10427_8.txt', '10118_7.txt', '10645_8.txt', '10000_8.txt', '10018_8.txt', '10532_8.txt', '10215_10.txt', '10339_7.txt', '10143_8.txt', '10552_9.txt', '10057_9.txt', '10529_10.txt', '10617_8.txt', '10045_10.txt', '10185_10.txt', '10445_10.txt', '10214_10.txt', '10139_8.txt', '10101_8.txt', '10071_9.txt', '10061_8.txt', '10524_10.txt', '10261_8.txt', '10309_7.txt', '10226_10.txt', '10099_10.txt', '10346_9.txt', '10452_10.txt', '1005_10.txt', '10264_10.txt', '10319_7.txt', '10276_10.txt', '10467_10.txt', '10025_9.txt', '10613_8.txt', '10335_8.txt', '110_10.txt', '10364_10.txt', '10344_7.txt', '10491_7.txt', '10295_7.txt', '10189_7.txt', '10178_10.txt', '10593_8.txt', '1013_9.txt', '10366_10.txt', '1049_7.txt', '1044_8.txt', '104_10.txt', '10280_10.txt', '10073_10.txt', '10383_10.txt', '10200_10.txt', '1060_10.txt', '10453_10.txt', '10363_9.txt', '10050_10.txt', '1053_8.txt', '10600_9.txt', '1091_10.txt', '10422_7.txt', '10128_9.txt', '1032_7.txt', '10513_7.txt', '10367_8.txt', '10151_8.txt', '10037_9.txt', '10235_8.txt', '1099_10.txt', '10272_10.txt', '10637_10.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Paths to positive and negative review directories\n",
        "neg_dir = os.path.join(extracted_path, \"data/neg/\")\n",
        "pos_dir = os.path.join(extracted_path, \"data/pos/\")\n",
        "\n",
        "# Load negative reviews\n",
        "neg_files = glob.glob(os.path.join(neg_dir, \"*.txt\"))\n",
        "neg_data = [{\"text\": open(file, \"r\", encoding=\"utf-8\").read(), \"sentiment\": 0} for file in neg_files]\n",
        "\n",
        "# Load positive reviews\n",
        "pos_files = glob.glob(os.path.join(pos_dir, \"*.txt\"))\n",
        "pos_data = [{\"text\": open(file, \"r\", encoding=\"utf-8\").read(), \"sentiment\": 1} for file in pos_files]\n",
        "\n",
        "# Combine into a single DataFrame\n",
        "import pandas as pd\n",
        "data = pd.DataFrame(neg_data + pos_data)\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())\n",
        "print(\"Dataset size:\", len(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJMWtfLOeG5o",
        "outputId": "0f4e3e20-33c6-4019-a186-cc5010002074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  sentiment\n",
            "0  One of the worst films I have ever seen. How t...          0\n",
            "1  The championship game is only a couple of days...          0\n",
            "2  People, please don't bother to watch this movi...          0\n",
            "3  A huge disappointment from writer Hamm and dir...          0\n",
            "4  Spoilers <br /><br />Well, the one line summar...          0\n",
            "Dataset size: 1319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Download punkt_tab data\n",
        "nltk.download('punkt_tab') # Download the necessary 'punkt_tab' data package\n",
        "\n",
        "# Initialize the lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags, URLs, and special characters\n",
        "    text = re.sub(r\"http\\S+|www\\S+|<.*?>\", \"\", text)  # Remove URLs and HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters and numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Remove stopwords and lemmatize\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows of the cleaned text\n",
        "print(data[['text', 'cleaned_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aWbOy4bembF",
        "outputId": "2cf93645-6bb6-447e-8093-67c1020a160d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  One of the worst films I have ever seen. How t...   \n",
            "1  The championship game is only a couple of days...   \n",
            "2  People, please don't bother to watch this movi...   \n",
            "3  A huge disappointment from writer Hamm and dir...   \n",
            "4  Spoilers <br /><br />Well, the one line summar...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  one worst film ever seen define worst would pr...  \n",
            "1  championship game couple day away thing new or...  \n",
            "2  people please dont bother watch movie movie ba...  \n",
            "3  huge disappointment writer hamm director dante...  \n",
            "4  spoiler well one line summary say melville le ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Use top 5000 features\n",
        "\n",
        "# Vectorize the cleaned text\n",
        "X = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
        "y = data['sentiment']  # Assuming sentiment column contains binary labels (e.g., 0 for negative, 1 for positive)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"TF-IDF Vectorization Complete!\")\n",
        "print(f\"Training Set Shape: {X_train.shape}\")\n",
        "print(f\"Testing Set Shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTpH9oO_fEeh",
        "outputId": "4573a7e4-900d-42fa-a4f7-dfdbab8b9cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectorization Complete!\n",
            "Training Set Shape: (1055, 5000)\n",
            "Testing Set Shape: (264, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{model_name} Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    results[model_name] = accuracy\n",
        "\n",
        "# Display best model\n",
        "best_model_name = max(results, key=results.get)\n",
        "print(f\"Best Model: {best_model_name} with Accuracy: {results[best_model_name] * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTS_kqp0fycz",
        "outputId": "39fba3e6-cfb0-4381-e0b0-d295829275d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 85.98%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.71      0.81       108\n",
            "           1       0.83      0.96      0.89       156\n",
            "\n",
            "    accuracy                           0.86       264\n",
            "   macro avg       0.88      0.84      0.85       264\n",
            "weighted avg       0.87      0.86      0.86       264\n",
            "\n",
            "Training SVM...\n",
            "SVM Accuracy: 89.02%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.81      0.86       108\n",
            "           1       0.88      0.95      0.91       156\n",
            "\n",
            "    accuracy                           0.89       264\n",
            "   macro avg       0.90      0.88      0.88       264\n",
            "weighted avg       0.89      0.89      0.89       264\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 80.68%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.64      0.73       108\n",
            "           1       0.79      0.92      0.85       156\n",
            "\n",
            "    accuracy                           0.81       264\n",
            "   macro avg       0.82      0.78      0.79       264\n",
            "weighted avg       0.81      0.81      0.80       264\n",
            "\n",
            "Training Naive Bayes...\n",
            "Naive Bayes Accuracy: 89.39%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.78      0.86       108\n",
            "           1       0.86      0.97      0.92       156\n",
            "\n",
            "    accuracy                           0.89       264\n",
            "   macro avg       0.91      0.88      0.89       264\n",
            "weighted avg       0.90      0.89      0.89       264\n",
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:41:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Accuracy: 78.41%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.69      0.72       108\n",
            "           1       0.80      0.85      0.82       156\n",
            "\n",
            "    accuracy                           0.78       264\n",
            "   macro avg       0.78      0.77      0.77       264\n",
            "weighted avg       0.78      0.78      0.78       264\n",
            "\n",
            "Best Model: Naive Bayes with Accuracy: 89.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('logreg', LogisticRegression()),\n",
        "    ('svc', SVC(probability=True)),\n",
        "    ('rf', RandomForestClassifier())\n",
        "]\n",
        "\n",
        "# Define stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the stacking classifier\n",
        "y_pred_stacking = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "print(classification_report(y_test, y_pred_stacking))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RkIXnsvf04t",
        "outputId": "6b284fa5-e101-481c-fae0-841f82a0bfb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 90.15%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88       108\n",
            "           1       0.93      0.90      0.92       156\n",
            "\n",
            "    accuracy                           0.90       264\n",
            "   macro avg       0.90      0.90      0.90       264\n",
            "weighted avg       0.90      0.90      0.90       264\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('logreg', LogisticRegression()),\n",
        "    ('svc', SVC(probability=True)),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('xgb', XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42))\n",
        "]\n",
        "\n",
        "# Define stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=3  # 3-fold cross-validation for stacking\n",
        ")\n",
        "\n",
        "# Train stacking model\n",
        "print(\"Training Stacking Classifier...\")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate stacking model\n",
        "y_pred_stacking = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stacking))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVahKiw-f8JO",
        "outputId": "66a5724a-e123-4e62-e0ba-9d7e414b3746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Stacking Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:42:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:42:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:42:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:43:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 90.15%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88       108\n",
            "           1       0.93      0.90      0.92       156\n",
            "\n",
            "    accuracy                           0.90       264\n",
            "   macro avg       0.90      0.90      0.90       264\n",
            "weighted avg       0.90      0.90      0.90       264\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Define models for voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('logreg', LogisticRegression()),\n",
        "        ('svc', SVC(probability=True)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('xgb', XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42))\n",
        "    ],\n",
        "    voting='soft'  # 'soft' for probability-based voting, 'hard' for label-based voting\n",
        ")\n",
        "\n",
        "# Train voting model\n",
        "print(\"Training Voting Classifier...\")\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate voting model\n",
        "y_pred_voting = voting_clf.predict(X_test)\n",
        "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
        "print(f\"Voting Classifier Accuracy: {voting_accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_voting))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr6PjJXGgMLM",
        "outputId": "a05689a8-6c9b-427a-fc9c-d9c17f9c4ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Voting Classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:43:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy: 89.39%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87       108\n",
            "           1       0.91      0.92      0.91       156\n",
            "\n",
            "    accuracy                           0.89       264\n",
            "   macro avg       0.89      0.89      0.89       264\n",
            "weighted avg       0.89      0.89      0.89       264\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbKqLixVgXd3",
        "outputId": "b5ba7f0b-4f9a-444c-acf1-5db3eb91c484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the CatBoost model\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=500,  # Number of boosting iterations\n",
        "    learning_rate=0.1,  # Learning rate\n",
        "    depth=6,  # Depth of the trees\n",
        "    eval_metric='Accuracy',\n",
        "    verbose=100,  # Verbosity during training\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "# Train CatBoost on the training set\n",
        "print(\"Training CatBoost Classifier...\")\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate CatBoost on the test set\n",
        "y_pred_catboost = catboost_model.predict(X_test)\n",
        "catboost_accuracy = accuracy_score(y_test, y_pred_catboost)\n",
        "print(f\"CatBoost Classifier Accuracy: {catboost_accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_catboost))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSKhVRHggcC_",
        "outputId": "c4d50933-43ea-41cb-dffd-47b12e7e1bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CatBoost Classifier...\n",
            "0:\tlearn: 0.7004739\ttotal: 428ms\tremaining: 3m 33s\n",
            "100:\tlearn: 0.9829384\ttotal: 18.1s\tremaining: 1m 11s\n",
            "200:\tlearn: 1.0000000\ttotal: 31.7s\tremaining: 47.2s\n",
            "300:\tlearn: 1.0000000\ttotal: 46.6s\tremaining: 30.8s\n",
            "400:\tlearn: 1.0000000\ttotal: 59.4s\tremaining: 14.7s\n",
            "499:\tlearn: 1.0000000\ttotal: 1m 12s\tremaining: 0us\n",
            "CatBoost Classifier Accuracy: 79.55%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.66      0.72       108\n",
            "           1       0.79      0.89      0.84       156\n",
            "\n",
            "    accuracy                           0.80       264\n",
            "   macro avg       0.80      0.77      0.78       264\n",
            "weighted avg       0.80      0.80      0.79       264\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('logreg', LogisticRegression()),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('catboost', CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, verbose=0, random_seed=42))\n",
        "]\n",
        "\n",
        "# Define stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=3  # 3-fold cross-validation\n",
        ")\n",
        "\n",
        "# Train stacking model\n",
        "print(\"Training Stacking Classifier with CatBoost...\")\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate stacking model\n",
        "y_pred_stacking = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Stacking Classifier (with CatBoost) Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stacking))\n"
      ],
      "metadata": {
        "id": "l9hxXgHiggPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}